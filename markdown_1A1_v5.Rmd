## Linear Model Selection and Regularization {.tabset .tabset-fade} 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#Use in other sections as necessary 
library(glmnet)
library(ISLR)
library(caret)
library(pls)
```

### Introduction 

</br></br>

In this team presentation, you will have the chance to learn more about ridge and lasso regression and their implications on model selection. You will also have the chance to look at PCR and PLS models and their implications. The former concepts will be explored in two seperate labs, followed by a guided exercise from An Introduction to Statistical Learning. 


Load `glmnet` and `ISLR` library

```{r, collapse=T, comment=NA, warning=F}
library(glmnet)
library(ISLR)
data("Hitters")
```

Objectives:
What parameters or features impact the salaries of major league baseball players?

The Data:
Real Data Set- Major League Baseball Data from the 1986 and 1987 seasons [322x20]

Dims: AtBat, Hits, Home Runs, Runs, Runs Batted in, Walks, Years (Player has been in Majors)
Career At Bats, Career Hits, Career Home Runs, Career Runs, Career Runs Batted in
Career Walks, League (American or National), Division (East or West), Put Outs,
Assists, Errors , Salary, and New League (Player's 1987 League)

From the ISLR Library

Other notes:
Some Null values that would need to be omitted, no transformations needed.  
More details can be found by using summary(Hitters) or str(Hitters)


```{r, collapse=T, comment=NA, warning=F}
sum(is.na(Hitters)) #59 N/A Values
HitData<-complete.cases(Hitters) # Figure out complete cases
HitData<-Hitters[HitData,] #Subset Complete cases
head(HitData)
#summary(HitData)
str(HitData) # 19 Predictors - 3 are 2 level so ~19 to start 
plot(Salary~Walks, data=Hitters)
plot(Salary~PutOuts, data=Hitters)
plot(Salary~Hits, data=Hitters)
plot(Salary~AtBat, data=Hitters)
```

We can also look at Multicollinearity using just a model using all factors and the 
lm() function and the vif() function from the car package. 

Doing so gives us 
some hints that some factors are correlated with each other which makes sense but something
we want to watch for as we use the Ridge, Lasso, PCR, and PCL methods 


### Ridge

</br></br>

**Create a Model**

Lets start by looking up what the model.matrix function does.

```{r, collapse=T, comment=NA, warning=F}
#?model.matrix
```
model.matrix creates a design (or model) matrix, e.g., 
by expanding factors to a set of dummy variables (depending on the contrasts) 
and expanding interactions similarly.


Using this function,we can set up our "x" using the matrix and "y" values using the Salary column from the Hitters data

```{r, collapse=T, comment=NA, warning=F}
x=model.matrix(Salary~.,HitData)[,-1]
y=HitData$Salary
```

**Create the model object**

We now will be using the glm package and glmnet() function to run the ridge
regression.  The glmnet() function uses a paramter "alpha"  setting this to 0 will
denote a **Ridge** regression while a 1 will denote a **Lasso**.

In this example we wil also be populating a **grid** of "lambda" values that control the
penalty term which is a factor of the coefficients (l2)

Ridge Regression (alpha= (0=Ridge or 1=Lasso))
```{r, collapse=T, comment=NA, warning=F}
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha = 0,lambda = grid)
COEF<-coef(ridge.mod)
dim(COEF)
```
We can see we have 20 coefficients and 100 potential lamda values 

***Exploring Lamda values***

We can explore the impact lamda and the penalty term have on the loss function by
looiking at different potential values of lambda.  Let's look at the 50th element.

**Large Lambda**
```{r, collapse=T, comment=NA, warning=F}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))# Find the L2 value
```

The L2 value is much smaller showing that the coefficients for the factors have been
shrunk


We can also look at the impact of a small lambda value and what that means for the 
beta coefficients 

**Small Lambda**
```{r, collapse=T, comment=NA, warning=F}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # Find the L2 value
```

The L2 value here is higher suggesting the coefficients for the factors are larger than they where for the larger lambda.

This makes logical sense as the value of lambda determines the magnitude of the L2 penalty term relative to the value of the
loss function as a whole.

***Predicting with a lambda value***

We can also predict for a new given value of lambda  by using the "s=" argument

```{r, collapse=T, comment=NA, warning=F}
predict(ridge.mod,s=50,type = "coefficients")[1:20,]
```


**Create Train & Test Data sets**

The next step involves creating test and training data sets.  The book example is
coded here, however the carat package could be used with the create data partition function
and restructuring our "y" and "x" values to pass to the glmnet() function.

```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
train<-sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
```

We can now try a model using the training set with lambda value of 4.
We then use the test data to get our MSE value to compare across models

```{r, collapse=T, comment=NA, warning=F}
ridge.mod<-glmnet(x[train,],y[train],alpha = 0,lambda = grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
```

Using a lambda of 4, we get an MSE of 142,199.2 which is slightly different from the book  due to the set seed function.

To test if this is any better than the "null" or no correlation hypothesis, we test the MSE
of a model with just the intercept (or a lambda value approaching infinity?)

**Null Hypothesis**

Use the mean of the y training set in place of the prediction
```{r, collapse=T, comment=NA, warning=F}
mean((mean(y[train])-y.test)^2)
# 224669.9 DIff from the book likely due to set seed.
```

We end up with an MSE of 224,669.9 which is worse than our lamda of 4's MSE.  

Let's try a large Lambda to see if it is indeed the same all coefficients going to zero.
```{r, collapse=T, comment=NA, warning=F}
ridge.pred<-predict(ridge.mod,s=1e10,newx = x[test,])
mean((ridge.pred-y.test)^2)
# 224669.8  Same as Null hypthosis as all coef go to zero for large Lambda
```

As we can see, we get the same value of 224,669.8

What about if we just used least squares?  This is the same as letting lambda go
to 0 as this removes the penalty term from the loss function.

```{r, collapse=T, comment=NA, warning=F}
ridge.pred<-predict(ridge.mod,s=0,newx = x[test,],newy=y.test) #Exact would not work
mean((ridge.pred-y.test)^2)
#167789.8
```

This gives us a value of 167,789.8 for our test MSE.  This is the number we'd like to improve upon 


*There are a lot of lambdas out there, how do we pick the best one?*

**Cross validation TO CHOOSE LAMBDA**

we will again use our glmnet() function, however, this time there won't be an s=
argument or a grid= argument.

The cv. modifier allows us to calculate the best lambda value as well as explore the impact
of different lambdas on the coefficients.

```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam #326.0828
ridge.pred<-predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)^2) # 139856.6
```

Our new test MSE is 139,856.6 and our ideal lamda is 326.0828


We can also explore what the coefficients look like 
using the whole data set and our best lamda value

```{r, collapse=T, comment=NA, warning=F}
out<-glmnet(x,y,alpha = 0)
predict(out,type = "coefficients",s=bestlam)[1:20,]
# none are zero
```
 
 We can see here that none of them are zero (as we expected) however their values 
 have been shrunk as our basic lm() model showed us, not all factors were significant in a full model.
 This can make the model a bit harder to interpret as we still have all our factors.
 
 
Next, we will look at the Lasso:

### Lasso

</br></br>

```{r, collapse=T, comment=NA, warning=F}
lasso.mod<-glmnet(x[train,],y[train],alpha = 1,lambda = grid)
plot(lasso.mod)
```
CV
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
cv.out<-cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam #  9.286955
```

```{r, collapse=T, comment=NA, warning=F}
lasso.pred<-predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) #143673.6
```

```{r, collapse=T, comment=NA, warning=F}
out=glmnet(x,y,alpha = 1,lambda = grid)
lasso.coef<-predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
```
runs, RBI, At Bat, Hits, Cwalks, Assists, and New League are all zero 



### PCR and PLS

</br></br>
Principal Components Regression (PCR)
PCR uses the first few components as the predictors in a linear regression model that is fit using least squares. We want to find out if a small number of principal components can explain most of the variability in the data.

PCR can be performed using `pcr()` function from `pls` library

Objectives: Predict salary

Load the `pls` library and run `pcr` to Hitters data.
We can set scale=TRUE to standardize each predictor.
```{r, collapse=T, comment=NA, warning=F}
library(pls)
set.seed(2)
#?pcr
pcr.fit=pcr(Salary~.,data=HitData,scale=TRUE,validation="CV")
```
Check the resulting fit using `summary()`
```{r, collapse=T, comment=NA, warning=F}
summary(pcr.fit)
```
See cross-validation
```{r, collapse=T, comment=NA, warning=F}
validationplot(pcr.fit,val.type = "MSEP")
```
We can see that when 16 components, the cross-validation error is the smallest
However, from the plot we also see that the cross-validation error is about the same with 1 component. 
This suggests that a model that uses just 1 component might be enough to explain.

Run PCR on the training data and check the performace
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
pcr.fit<-pcr(Salary~.,data=HitData,subset=train,scale=TRUE,validation="CV")
summary(pcr.fit)
```
Lowest cross-validation error when 5 components used

```{r, collapse=T, comment=NA, warning=F}
validationplot(pcr.fit,val.type = "MSEP")
```
Test
```{r, collapse=T, comment=NA, warning=F}
pcr.pred=predict(pcr.fit,x[test,],ncomp=5)
mean((pcr.pred-y.test)^2) #142811.8
```

PCR on the full data set
```{r, collapse=T, comment=NA, warning=F}
pcr.fit<-pcr(y~x,scale=TRUE,ncomp=5)
summary(pcr.fit)
```

Partial Least Squares (PLS)


PLS can be performed using `plsr()` function in the `pls` library. 
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
pls.fit<-plsr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
summary(pls.fit)
```
```{r, collapse=T, comment=NA, warning=F}
validationplot(pls.fit,val.type="MSEP")
```
Lowest cross-validation error when only 2 pls are used.

Check MSE
```{r, collapse=T, comment=NA, warning=F}
pls.pred<-predict(pls.fit,x[test,],ncomp = 2)
mean((pls.pred-y.test)^2)# 125968.2
```

PLS using the full data set
```{r, collapse=T, comment=NA, warning=F}
pls.fit<-plsr(Salary~., data=HitData,scale=TRUE,ncomp=2)
summary(pls.fit)
```

### Application Exercise
</br></br>

**Objectives:**
Identify what attributes/ features of American colleges are the most important in predicting the number of applicants a college receives.
Use lasso, ridge, principle compenent, and partial least squares regression to find model that minimizes MSE and maximizes R^2.

**Data Source:**
Real Data Set.  1995 issue of US News and World Report [777x18]

**Dimensions:** 
Private, Apps, Accept, Enroll, Top10perc, Top25perc, F.Undergrad, P.Undergrad, Outstate
Room.Board, Books, Personal, PhD, Terminal, S.F. Ratio, perc.alumni, Expend, Grad.Rate

**Exploring the Data:**

```{r, collapse=T, comment=NA, warning=F}
library(ISLR)
library(caret)
library(glmnet)

set.seed(1)
College <- ISLR::College
sum(is.na(College)) #0 N/A Values
head(College)
#summary(HitData)
str(College) 
plot(Apps~Enroll, data = College)
plot(Apps~Outstate, data = College)
plot(Apps~Accept, data = College)
plot(Apps~F.Undergrad, data=College)
```

### Ridge and Lasso (Applied Exercise)
</br></br>

a. Split the data set into a training set and a test set using caret's `createDataPartition()` function.
```{r, collapse=T, comment=NA, warning=F}
divideData <- createDataPartition(College$Apps, p=.8,list = F)
train<- College[divideData,]
test<- College[-divideData,]
```

</br>

b. Fit a linear model using least squares on the training set, and
report the test error obtained.
```{r, collapse=T, comment=NA, warning=F}
linear <- lm(Apps~., data=train)
summary(linear)
```

From the data, we are able to find the MSE for the linear model: 

```{r,collapse=T, comment=NA, warning=F,echo=FALSE}
prediction <- predict(linear, test)
lmerror <- mean((test$Apps-prediction)^2)
lmerror
```

</br>

c. Fit a ridge regression model on the training set, with λ chosen
by cross-validation. Report the test error obtained.

First, we want to create two matrices using our `train` and `test` data. 

```{r, collapse=T, comment=NA, warning=F}
train_matrix <- model.matrix(Apps~.,data=train)
test_matrix <- model.matrix(Apps~.,data=test)
```

**Model Creation and Lambda**

Next, we want to form a ridge model by using the `glmnet` library. The function `glmnet()` will "fit a generalized linear model via penalized maximum likelihood" using lambda; the `alpha=0` below indicates the ridge penalty, set as the value 0 (conversly, a 1 will be used for the lasso penalty). . We also want to generate a range of different lambda values that will adequately produce our ideal lambda. In order to do this, we can use cross-validation on our model to eventually find the lowest MSE value possible by the lambda we find. After using `cv.glmnet()`, we are able use `$lambda.min` from the `glmnet` library on the cross validation version of the model to determine the best lambda. 

```{r,collapse=T, comment=NA, warning=F}
grid=10^seq(10,-2,length=100)

ridge <- glmnet(train_matrix, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
crossv <- cv.glmnet(train_matrix, train$Apps, alpha=0)

lambda_ridge <- crossv$lambda.min
lambda_ridge
```

We find that the best lambda is 369.84. We are finally able to make a prediction using the `test` matrix (`newx=`), ridge model, and lambda of 369 (`s=`) to find the minimized MSE of the model:

```{r,collapse=T, comment=NA, warning=F}
ridge_prediction <- predict(ridge, s=lambda_ridge, newx = test_matrix)
ridge_error <- mean((test$Apps-ridge_prediction)^2)
ridge_error
```

We can see that the MSE is much lower than the linear model, lower by 219,621! It would seem that the model accuracy for the ridge model is substantially better than the linear model. What about the lasso method? 


</br>

d. Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.

**Model Creation and Lambda**

For the lasso method, we will be using the same matrices, `train_matrix` and `train_matrix`, for the creation of the model.

The lasso model uses a similar approach as the ridge model setup using the `glmnet` library. The function `glmnet()` will "fit a generalized linear model via penalized maximum likelihood" using lambda; the `alpha=1` is set for a lasso model. Similar to the last model, we want to use generated lambda values and use cross-validation to find an ideal lambda value for the model. After using `cv.glmnet()`, we are able use `$lambda.min` from the `glmnet` library on the cross validation version of the model to determine the best lambda. 

```{r, collapse=T, comment=NA, warning=F}
lasso <- glmnet(train_matrix, train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
crossv_2 <- cv.glmnet(train_matrix,train$Apps, alpha=1)

lambda_lasso <- crossv_2$lambda.min #check coeff for lamba (how many or 0)
lambda_lasso
```
We receive a value of 2.61 as our lambda. Along with our new values for the lasso model, we can predict the MSE test value:

```{r}
lasso_prediction <- predict(lasso, s=lambda_lasso, newx = test_matrix)
lasso_error <- mean((test$Apps-lasso_prediction)^2)
lasso_error
```

** Possible causes for the Model Performance ** 

Interestingly, the lasso model performs worse than the ridge model. Assuming our lambda value is the ideal lambda model, there may be other factors in the dataset that could affect the outcomes of each model. 

One potential problem with ridge models compared to other models is that all predictors $p$ will be included in the model. This means if that there are useless predictor variables, there may be issues with overfitting the model. Lasso models seek to mitigate this problem by reducing the beta coefficients like ridge models. The key difference is that lasso models will bring certain coefficients to 0 with a high enough lambda. In other words, the lasso approach using variable selection to omit certain variables that may be irrelevant. 

</br>

Looking at the coefficient values for Lasso approach: 

```{r, collapse=T, comment=NA, warning=F}
predict(lasso,s=lambda_lasso,type="coefficients")
```

</br>

### PLS/PCR (Applied Exercise)
</br></br>

**Dimension Reduction Methods**

e. Fit a PCR model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.

Principal components are linear combinations of variables that have high varaince. Principal components shouldn't be correlated with each other.
```{r, collapse=T, comment=NA, warning=F}
library(pls)
set.seed(1)
pcr <- pcr(Apps~.,data=train,scale=TRUE, validation='CV')
validationplot(pcr,val.type="MSEP")
pcr_prediction <- predict(pcr, test, ncomp = 17)
pcr_error <- mean((test$Apps-pcr_prediction)^2)
pcr_error
```

</br>

f. Fit a PLS model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.

Partial least squares (PLS) is a supervised alternative to principle components regression(PCR).
```{r, collapse=T, comment=NA, warning=F}
pls <- plsr(Apps~.,data=train, scale=TRUE, validation='CV')
validationplot(pls,val.type = "MSEP")
pls_prediction <- predict(pls, test, ncomp = 10)
pls_error <- mean((test$Apps-pls_prediction)^2)
pls_error
```


### Conclusion (Applied Exercise)
</br></br>
g. Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

For this example, we will be using the predicition data from each model and comparing the $R^2$ values of each model. The model with the highest $R^2$ value represents a better model accuracy. Finally, we compare the MSE values and $R^2$ values of all models together in a table. 

```{r, collapse=T, comment=NA, warning=F}
test_avg <- mean(test$Apps)
#Least square
linear_r2 <- 1 - mean((prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#Ridge model
ridge_r2 <- 1 - mean((ridge_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#Lasso model
lasso_r2 <- 1 - mean((lasso_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#PCR model
pcr_r2 <- 1 - mean((pcr_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#PLS model
pls_r2 <- 1 - mean((pls_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
```

```{r,collapse=T, comment=NA, warning=F, echo=F}
errors <- as.table(rbind(c(lmerror,linear_r2),c(ridge_error,ridge_r2),c(lasso_error,lasso_r2),c(pcr_error,pcr_r2),c(pls_error,pls_r2)))
dimnames(errors) <- list(Model_Type = c("Linear", "Ridge", "Lasso", "PCR", "PLS"), Error_Measures = c("MSE", "R-Squared"))
format(errors, scientific = F)
```
```{r, collapse=T, comment=NA, warning=F, echo=F}
sprintf("The lowest MSE value and highest R-Square value come from the Ridge Regression, with an MSE of %f and a R-Square of %f.", ridge_error,ridge_r2)
```
