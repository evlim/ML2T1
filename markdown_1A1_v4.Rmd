## Linear Model Selection and Regularization {.tabset .tabset-fade} 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#Use in other sections as necessary 
library(glmnet)
library(ISLR)
library(caret)
library(pls)
```

### Introduction 

</br></br>

In this team presentation, you will have the chance to learn more about ridge and lasso regression and their implications on model selection. You will also have the chance to look at PCR and PLS models and their implications. The former concepts will be explored in two seperate labs, followed by a guided exercise from An Introduction to Statistical Learning. 



Load `glmnet` and `ISLR` library

```{r, collapse=T, comment=NA, warning=F}
library(glmnet)
library(ISLR)
data("Hitters")
```

Objectives:
What parameters or features impact the salaries of major league baseball players?

The Data:
Real Data Set- Major League Baseball Data from the 1986 and 1987 seasons [322x20]

Dims: AtBat, Hits, Home Runs, Runs, Runs Batted in, Walks, Years (Player has been in Majors)
Career At Bats, Career Hits, Career Home Runs, Career Runs, Career Runs Batted in
Career Walks, League (American or National), Division (East or West), Put Outs,
Assists, Errors , Salary, and New League (Player's 1987 League)

From the ISLR Library

Other notes:
Some Null values that would need to be omitted, no transformations needed.  
More details can be found by using summary(Hitters) or str(Hitters)



```{r, collapse=T, comment=NA, warning=F}
sum(is.na(Hitters)) #59 N/A Values
HitData<-complete.cases(Hitters) # Figure out complete cases
HitData<-Hitters[HitData,] #Subset Complete cases
head(HitData)
#summary(HitData)
str(HitData) # 19 Predictors - 3 are 2 level so ~19 to start 
plot(Salary~Walks, data=Hitters)
plot(Salary~PutOuts, data=Hitters)
plot(Salary~Hits, data=Hitters)
plot(Salary~AtBat, data=Hitters)
```

We can also look at Multicollinearity using just a model using all factors and the 
lm() function and the vif() function from the car package. 

Doing so gives us 
some hints that some factors are correlated with each other which makes sense but something
we want to watch for as we use the Ridge, Lasso, PCR, and PCL methods 


### Ridge

</br></br>

**Create a Model**

Lets start by looking up what the model.matrix function does.

```{r, collapse=T, comment=NA, warning=F}
#?model.matrix
```
model.matrix creates a design (or model) matrix, e.g., 
by expanding factors to a set of dummy variables (depending on the contrasts) 
and expanding interactions similarly.


Using this function,we can set up our "x" using the matrix and "y" values using the Salary column from the Hitters data

```{r, collapse=T, comment=NA, warning=F}
x=model.matrix(Salary~.,HitData)[,-1]
y=HitData$Salary
```

***Create the model object***

We now will be using the glm package and glmnet() function to run the ridge
regression.  The glmnet() function uses a paramter "alpha"  setting this to 0 will
denote a **Ridge** regression while a 1 will denote a **Lasso**.

In this example we wil also be populating a **grid** of "lambda" values that control the
penalty term which is a factor of the coefficients (l2)

Ridge Regression (alpha= (0=Ridge or 1=Lasso))
```{r, collapse=T, comment=NA, warning=F}
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha = 0,lambda = grid)
COEF<-coef(ridge.mod)
dim(COEF)
```
We can see we have 20 coefficients and 100 potential lamda values 

***Exploring Lamda values***

We can explore the impact lamda and the penalty term have on the loss function by
looiking at different potential values of lambda.  Let's look at the 50th element.

**Large Lambda**
```{r, collapse=T, comment=NA, warning=F}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))# Find the L2 value
```

The L2 value is much smaller showing that the coefficients for the factors have been
shrunk


We can also look at the impact of a small lambda value and what that means for the 
beta coefficients 

**Small Lambda**
```{r, collapse=T, comment=NA, warning=F}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # Find the L2 value
```

The L2 value here is higher suggesting the coefficients for the factors are larger than they where for the larger lambda.

This makes logical sense as the value of lambda determines the magnitude of the L2 penalty term relative to the value of the
loss function as a whole.

***Predicting with a lambda value***

We can also predict for a new given value of lambda  by using the "s=" argument

```{r, collapse=T, comment=NA, warning=F}
predict(ridge.mod,s=50,type = "coefficients")[1:20,]
```


**Create Train & Test Data sets**

The next step involves creating test and training data sets.  The book example is
coded here, however the carat package could be used with the create data partition function
and restructuring our "y" and "x" values to pass to the glmnet() function.

```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
train<-sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
```

We can now try a model using the training set with lambda value of 4.
We then use the test data to get our MSE value to compare across models

```{r, collapse=T, comment=NA, warning=F}
ridge.mod<-glmnet(x[train,],y[train],alpha = 0,lambda = grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
```

Using a lambda of 4, we get an MSE of 142,199.2 which is slightly different from the book  due to the set seed function.

To test if this is any better than the "null" or no correlation hypothesis, we test the MSE
of a model with just the intercept (or a lambda value approaching infinity?)

**null Hypothesis**

Use the mean of the y training set in place of the prediction
```{r, collapse=T, comment=NA, warning=F}
mean((mean(y[train])-y.test)^2)
# 224669.9 DIff from the book likely due to set seed.
```

We end up with an MSE of 224,669.9 which is worse than our lamda of 4's MSE.  

Let's try a large Lambda to see if it is indeed the same all coefficients going to zero.
```{r, collapse=T, comment=NA, warning=F}
ridge.pred<-predict(ridge.mod,s=1e10,newx = x[test,])
mean((ridge.pred-y.test)^2)
# 224669.8  Same as Null hypthosis as all coef go to zero for large Lambda
```

As we can see, we get the same value of 224,669.8

What about if we just used least squares?  This is the same as letting lambda go
to 0 as this removes the penalty term from the loss function.

```{r, collapse=T, comment=NA, warning=F}
ridge.pred<-predict(ridge.mod,s=0,newx = x[test,],newy=y.test) #Exact would not work
mean((ridge.pred-y.test)^2)
#167789.8
```

This gives us a value of 167,789.8 for our test MSE.  This is the number we'd like to improve upon 


*There are a lot of lambdas out there, how do we pick the best one?*

**Cross validation TO CHOOSE LAMBDA**

we will again use our glmnet() function, however, this time there won't be an s=
argument or a grid= argument.

The cv. modifier allows us to calculate the best lambda value as well as explore the impact
of different lambdas on the coefficients.

```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam #326.0828
ridge.pred<-predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)^2) # 139856.6
```

Our new test MSE is 139,856.6 and our ideal lamda is 326.0828


We can also explore what the coefficients look like 
using the whole data set and our best lamda value

```{r, collapse=T, comment=NA, warning=F}
out<-glmnet(x,y,alpha = 0)
predict(out,type = "coefficients",s=bestlam)[1:20,]
# none are zero
```
 
 We can see here that none of them are zero (as we expected) however their values 
 have been shrunk as our basic lm() model showed us, not all factors were significant in a full model.
 This can make the model a bit harder to interpret as we still have all our factors.
 
 
Next, we will look at the Lasso:

### Lasso

</br></br>

```{r, collapse=T, comment=NA, warning=F}
lasso.mod<-glmnet(x[train,],y[train],alpha = 1,lambda = grid)
plot(lasso.mod)
```
CV
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
cv.out<-cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam #  9.286955
```

```{r, collapse=T, comment=NA, warning=F}
lasso.pred<-predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) #143673.6
```

```{r, collapse=T, comment=NA, warning=F}
out=glmnet(x,y,alpha = 1,lambda = grid)
lasso.coef<-predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
```
runs, RBI, At Bat, Hits, Cwalks, Assists, and New League are all zero 



### PCR and PLS

</br></br>

Principal Components Regression (PCR)

Load the `pls` library:

```{r, collapse=T, comment=NA, warning=F}
library(pls)
set.seed(2)
#?pcr
pcr.fit=pcr(Salary~.,data=HitData,scale=TRUE,validation="CV")
summary(pcr.fit)
```
See validation
```{r, collapse=T, comment=NA, warning=F}
validationplot(pcr.fit,val.type = "MSEP")
```
Test performace
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
pcr.fit<-pcr(Salary~.,data=HitData,subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type = "MSEP")
```
Test
```{r, collapse=T, comment=NA, warning=F}
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2) #140751.3
```

```{r, collapse=T, comment=NA, warning=F}
```
All data
```{r, collapse=T, comment=NA, warning=F}
pcr.fit<-pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
```

Partial Least Squares (PLS)

```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
pls.fit<-plsr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
```

MSE

```{r, collapse=T, comment=NA, warning=F}
pls.pred<-predict(pls.fit,x[test,],ncomp = 2)
mean((pls.pred-y.test)^2)# 125968.2
```

```{r, collapse=T, comment=NA, warning=F}
pls.fit<-plsr(Salary~., data=HitData,scale=TRUE,ncomp=2)
summary(pls.fit)
```

### Application Exercise
</br></br>

**Objectives:**
Identify what attributes/ features of American colleges are the most important in predicting the number of applicants a college receives.
Use lasso, ridge, principle compenent, and partial least squares regression to find model that minimizes MSE and maximizes R^2.

**Data Source:**
Real Data Set.  1995 issue of US News and World Report [777x18]

**Dimensions:** 
Private, Apps, Accept, Enroll, Top10perc, Top25perc, F.Undergrad, P.Undergrad, Outstate
Room.Board, Books, Personal, PhD, Terminal, S.F. Ratio, perc.alumni, Expend, Grad.Rate

**Exploring the Data:**

```{r, collapse=T, comment=NA, warning=F}
sum(is.na(College)) #0 N/A Values
head(College)
#summary(HitData)
str(College) 
plot(Apps~Enroll, data = College)
plot(Apps~Outstate, data = College)
plot(Apps~Accept, data = College)
plot(Apps~F.Undergrad, data=College)
```



### Ridge and Lasso (Applied Exercise)
</br></br>

First, we load the dataset and all requisite libraries(`ISLR`,`caret`,`glmnet`)

```{r, collapse=T, comment=NA, warning=F}
library(ISLR)
library(caret)
library(glmnet)
set.seed(1)
College <- ISLR::College
```

a. Split the data set into a training set and a test set using caret's `createDataPartition()` function.
```{r, collapse=T, comment=NA, warning=F}
divideData <- createDataPartition(College$Apps, p=.8,list = F)
train<- College[divideData,]
test<- College[-divideData,]
```

</br>

b. Fit a linear model using least squares on the training set, and
report the test error obtained.
```{r, collapse=T, comment=NA, warning=F}
linear <- lm(Apps~., data=train)
prediction <- predict(linear, test)
lmerror <- mean((test$Apps-prediction)^2)
lmerror
```

</br>

c. Fit a ridge regression model on the training set, with λ chosen
by cross-validation. Report the test error obtained.
```{r, collapse=T, comment=NA, warning=F}
train_matrix <- model.matrix(Apps~.,data=train)
test_matrix <- model.matrix(Apps~.,data=test)
grid=10^seq(10,-2,length=100)
ridge <- glmnet(train_matrix, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
crossv <- cv.glmnet(train_matrix, train$Apps, alpha=0)
lambda_ridge <- crossv$lambda.min
lambda_ridge
ridge_prediction <- predict(ridge, s=lambda_ridge, newx = test_matrix)
mean((test$Apps-ridge_prediction)^2)
```

</br>

d. Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.
```{r, collapse=T, comment=NA, warning=F}
lasso <- glmnet(train_matrix, train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
crossv_2 <- cv.glmnet(train_matrix,train$Apps, alpha=1)
lambda_lasso <- crossv_2$lambda.min #check coeff for lamba (how many or 0)
lambda_lasso
lasso_prediction <- predict(lasso, s=lambda_lasso, newx = test_matrix)
mean((test$Apps-lasso_prediction)^2)
```

</br>
Looking at the coefficient values: 

```{r, collapse=T, comment=NA, warning=F}
predict(lasso,s=lambda_lasso,type="coefficients")
```

</br>

### PCR/PLS (Applied Exercise)
</br></br>

**Dimension Reduction Methods**

e. Fit a PCR model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.

Principal components are linear combinations of variables that have high varaince. Principal components shouldn't be correlated with each other.
```{r, collapse=T, comment=NA, warning=F}
library(pls)
set.seed(1)
pcr <- pcr(Apps~.,data=train,scale=TRUE, validation='CV')
validationplot(pcr,val.type="MSEP")
pcr_prediction <- predict(pcr, test, ncomp = 17)
mean((test$Apps-pcr_prediction)^2)
```

</br>

f. Fit a PLS model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.

Partial least squares (PLS) is a supervised alternative to principle components regression(PCR).
```{r, collapse=T, comment=NA, warning=F}
pls <- plsr(Apps~.,data=train, scale=TRUE, validation='CV')
validationplot(pls,val.type = "MSEP")
pls_prediction <- predict(pls, test, ncomp = 10)
mean((test$Apps-pls_prediction)^2)
```


### Conclusion (Applied Exercise)
</br></br>
g. Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

For this example, we will be using the predicition data from each model and comparing the $R^2$ values of each model. The model with the highest $R^2$ value represents a better accuracy.  

```{r, collapse=T, comment=NA, warning=F}
test_avg <- mean(test$Apps)
#Least square
linear_r2 <- 1 - mean((prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#Ridge model
ridge_r2 <- 1 - mean((ridge_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#Lasso model
lasso_r2 <- 1 - mean((lasso_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#PCR model
pcr_r2 <- 1 - mean((pcr_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#PLS model
pls_r2 <- 1 - mean((pls_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
```

```{r, collapse=T, comment=NA, warning=F, echo=F}
sprintf("The Least Square R-Square value is %f", linear_r2)
sprintf("The Ridge R-Square value is %f",ridge_r2)
sprintf("The Lasso R-Square value is %f",lasso_r2)
sprintf("The PCR R-Square value is %f",pcr_r2)
sprintf("The PLS Least Square R-Square value is %f",pls_r2)
```


### Review
</br></br>