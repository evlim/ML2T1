
## Linear Model Selection and Regularization {.tabset .tabset-fade} 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#Use in other sections as necessary 
library(glmnet)
library(ISLR)
library(caret)
library(pls)
```

### Introduction 

</br></br>

In this team presentation, you will have the chance to learn more about ridge and lasso regression and their implications on model selection. You will also have the chance to look at PCR and PLS models and their implications. The former concepts will be explored in two seperate labs, followed by a guided exercise from An Introduction to Statistical Learning. 

### Ridge

</br></br>

Load `glmnet` and `ISLR` library

```{r, collapse=T, comment=NA, warning=F}
library(glmnet)
library(ISLR)
data("Hitters")
```

Summarize the data, then looks for any discrepencies (NA's, redundancies, etc.)

```{r, collapse=T, comment=NA, warning=F}
sum(is.na(Hitters)) #59 N/A Values
HitData<-complete.cases(Hitters) # Figure out complete cases
HitData<-Hitters[HitData,] #Subset Complete cases
head(HitData)
summary(HitData)
str(HitData) # 19 Predictors - 3 are 2 level so ~19 to start 
```

Create Model

```{r, collapse=T, comment=NA, warning=F}
?model.matrix
x=model.matrix(Salary~.,HitData)[,-1]
str(x) # Still 19 since 3 factors are dummy 1/0's

y=HitData$Salary
```

Ridge Regression (Alpha (0=Ridge, 1=Lasso))
```{r, collapse=T, comment=NA, warning=F}
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha = 0,lambda = grid)

COEF<-coef(ridge.mod)
dim(COEF)
```
Large Lambda
```{r, collapse=T, comment=NA, warning=F}
ridge.mod$lambda[50]

coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```

Small Lambda
```{r, collapse=T, comment=NA, warning=F}
ridge.mod$lambda[60]

coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```
Predict for a new lambda 
```{r, collapse=T, comment=NA, warning=F}
predict(ridge.mod,s=50,type = "coefficients")[1:20,]
```
Create Train & Test Data sets
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
train<-sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
```
Training set with lambda of 4
```{r, collapse=T, comment=NA, warning=F}
ridge.mod<-glmnet(x[train,],y[train],alpha = 0,lambda = grid, thresh=1e-12)
ridge.pred<-predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
```
142199.2 DIff from the book likely due to set seed.

null Hypothesis
```{r, collapse=T, comment=NA, warning=F}
mean((mean(y[train])-y.test)^2)
# 224669.9 DIff from the book likely due to set seed.
```

Try a Large Lambda
```{r, collapse=T, comment=NA, warning=F}
ridge.pred<-predict(ridge.mod,s=1e10,newx = x[test,])
mean((ridge.pred-y.test)^2)
# 224669.8  Same as Null hypthosis as all coef go to zero for large Lambda
```
Try just least squares

Try a Large Lambda
```{r, collapse=T, comment=NA, warning=F}
ridge.pred<-predict(ridge.mod,s=0,newx = x[test,],newy=y.test) #Exact would not work
mean((ridge.pred-y.test)^2)
#167789.8
```
CV TO CHOOSE LAMBDA
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam #326.0828

ridge.pred<-predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)^2) # 139856.6
```
Use on whole data set
```{r, collapse=T, comment=NA, warning=F}
out<-glmnet(x,y,alpha = 0)
predict(out,type = "coefficients",s=bestlam)[1:20,]
# none are zero
```

Next, we will look at the Lasso:

### Lasso

</br></br>

```{r, collapse=T, comment=NA, warning=F}
lasso.mod<-glmnet(x[train,],y[train],alpha = 1,lambda = grid)
plot(lasso.mod)
```
CV
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
cv.out<-cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam #  9.286955
```

```{r, collapse=T, comment=NA, warning=F}
lasso.pred<-predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) #143673.6
```

```{r, collapse=T, comment=NA, warning=F}
out=glmnet(x,y,alpha = 1,lambda = grid)
lasso.coef<-predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
```
runs, RBI, At Bat, Hits, Cwalks, Assists, and New League are all zero 



### PCR and PLS

</br></br>

Principal Components Regression (PCR)

Load the `pls` library:

```{r, collapse=T, comment=NA, warning=F}
library(pls)
set.seed(2)

#?pcr
pcr.fit=pcr(Salary~.,data=HitData,scale=TRUE,validation="CV")
summary(pcr.fit)
```
See validation
```{r, collapse=T, comment=NA, warning=F}
validationplot(pcr.fit,val.type = "MSEP")
```
Test performace
```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
pcr.fit<-pcr(Salary~.,data=HitData,subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type = "MSEP")
```
Test
```{r, collapse=T, comment=NA, warning=F}
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2) #140751.3
```

```{r, collapse=T, comment=NA, warning=F}

```
All data
```{r, collapse=T, comment=NA, warning=F}
pcr.fit<-pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
```

Partial Least Squares (PLS)

```{r, collapse=T, comment=NA, warning=F}
set.seed(1)
pls.fit<-plsr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
```

MSE

```{r, collapse=T, comment=NA, warning=F}
pls.pred<-predict(pls.fit,x[test,],ncomp = 2)
mean((pls.pred-y.test)^2)# 125968.2
```

```{r, collapse=T, comment=NA, warning=F}
pls.fit<-plsr(Salary~., data=HitData,scale=TRUE,ncomp=2)
summary(pls.fit)
```

### Application Exercise
</br></br>


### Ridge and Lasso (Applied Exercise)
</br></br>

First, we load the dataset and all requisite libraries(`ISLR`,`caret`,`glmnet`)

```{r, collapse=T, comment=NA, warning=F}
library(ISLR)
library(caret)
library(glmnet)

set.seed(1)
College <- ISLR::College
```

a. Split the data set into a training set and a test set using caret's `createDataPartition()` function.
```{r, collapse=T, comment=NA, warning=F}
divideData <- createDataPartition(College$Apps, p=.8,list = F)
train<- College[divideData,]
test<- College[-divideData,]
```

</br>

b. Fit a linear model using least squares on the training set, and
report the test error obtained.
```{r, collapse=T, comment=NA, warning=F}
linear <- lm(Apps~., data=train)

prediction <- predict(linear, test)

lmerror <- mean((test$Apps-prediction)^2)
lmerror
```

</br>

c. Fit a ridge regression model on the training set, with λ chosen
by cross-validation. Report the test error obtained.
```{r, collapse=T, comment=NA, warning=F}
train_matrix <- model.matrix(Apps~.,data=train)
test_matrix <- model.matrix(Apps~.,data=test)

grid=10^seq(10,-2,length=100)

ridge <- glmnet(train_matrix, train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)

crossv <- cv.glmnet(train_matrix, train$Apps, alpha=0)

lambda_ridge <- crossv$lambda.min
lambda_ridge

ridge_prediction <- predict(ridge, s=lambda_ridge, newx = test_matrix)

mean((test$Apps-ridge_prediction)^2)
```

</br>

d. Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.
```{r, collapse=T, comment=NA, warning=F}
lasso <- glmnet(train_matrix, train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)

crossv_2 <- cv.glmnet(train_matrix,train$Apps, alpha=1)

lambda_lasso <- crossv_2$lambda.min #check coeff for lamba (how many or 0)
lambda_lasso

lasso_prediction <- predict(lasso, s=lambda_lasso, newx = test_matrix)

mean((test$Apps-lasso_prediction)^2)
```

</br>
Looking at the coefficient values: 

```{r, collapse=T, comment=NA, warning=F}
predict(lasso,s=lambda_lasso,type="coefficients")
```

</br>

### PLS/PCR (Applied Exercise)
</br></br>

We will be using the package `pls` 
e. Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.
```{r, collapse=T, comment=NA, warning=F}
library(pls)
set.seed(1)

pcr <- pcr(Apps~.,data=train,scale=T, validation='CV')
validationplot(pcr,val.type="MSEP")

pcr_prediction <- predict(pcr, test, ncomp = 17)
mean((test$Apps-pcr_prediction)^2)
```

</br>

f. Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r, collapse=T, comment=NA, warning=F}
pls <- plsr(Apps~.,data=train, scale=T, validation='CV')
validationplot(pls,val.type = "MSEP")

pls_prediction <- predict(pls, test, ncomp = 10)
mean((test$Apps-pls_prediction)^2)
```


### Conclusion (Applied Exercise)
</br></br>
g. Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

For this example, we will be using the predicition data from each model and comparing the $R^2$ values of each model. The model with the highest $R^2$ value represents a better accuracy.  

```{r, collapse=T, comment=NA, warning=F}

test_avg <- mean(test$Apps)

#Least square
linear_r2 <- 1 - mean((prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#Ridge model
ridge_r2 <- 1 - mean((ridge_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#Lasso model
lasso_r2 <- 1 - mean((lasso_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#PCR model
pcr_r2 <- 1 - mean((pcr_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
#PLS model
pls_r2 <- 1 - mean((pls_prediction - test$Apps)^2) / mean((test_avg - test$Apps)^2)
```

```{r, collapse=T, comment=NA, warning=F, echo=F}
sprintf("The Least Square R-Square value is %f", linear_r2)
sprintf("The Ridge R-Square value is %f",ridge_r2)
sprintf("The Lasso R-Square value is %f",lasso_r2)
sprintf("The PCR R-Square value is %f",pcr_r2)
sprintf("The PLS Least Square R-Square value is %f",pls_r2)
```


### Review
</br></br>

